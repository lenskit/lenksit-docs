<!DOCTYPE html>
<html class="writer-html5" lang="en"><head>
  <meta charset="utf-8"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Top-N Evaluation — LensKit 0.14.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css">
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css">
    <link rel="canonical" href="https://lkpy.lenskit.org/stable/evaluation/topn-metrics.html">
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html">
    <link rel="search" title="Search" href="../search.html">
    <link rel="next" title="Algorithm Interfaces" href="../interfaces.html">
    <link rel="prev" title="Prediction Accuracy Metrics" href="predict-metrics.html"> 

<!-- RTD Extra Head -->



<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": false, "api_host": "https://readthedocs.org", "build_date": "2022-02-19T19:12:10Z", "builder": "sphinx", "canonical_url": null, "commit": "9e471a65", "docroot": "/docs/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "evaluation/topn-metrics", "programming_language": "py", "project": "lkpy", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {"lenskit-hpf": "https://lkpy.readthedocs.io/projects/lenskit-hpf/en/latest/", "lenskit-implicit": "https://lkpy.readthedocs.io/projects/lenskit-implicit/en/latest/", "lenskit-tf": "https://lkpy.readthedocs.io/projects/lenskit-tf/en/latest/"}, "theme": "sphinx_rtd_theme", "user_analytics_code": "", "version": "0.14.0"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="https://assets.readthedocs.org/static/javascript/readthedocs-analytics.js" async="async"></script>

<!-- end RTD <extrahead> -->
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../index.html" class="icon icon-home"> LensKit
          </a>
              <div class="version">
                0.14.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs">
    <input type="hidden" name="check_keywords" value="yes">
    <input type="hidden" name="area" value="default">
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install LensKit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/lenskit/lkpy/releases">Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Running Experiments</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Loading Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../crossfold.html">Splitting Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../batch.html">Batch-Running Recommenders</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Evaluating Recommender Output</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="predict-metrics.html">Prediction Accuracy Metrics</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Top-<em>N</em> Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-lenskit.topn">Top-<em>N</em> Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-lenskit.metrics.topn">Metrics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#classification-metrics">Classification Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ranked-list-metrics">Ranked List Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#utility-metrics">Utility Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#writing-a-metric">Writing a Metric</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#saving-and-loading-outputs">Saving and Loading Outputs</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../interfaces.html">Algorithm Interfaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms.html">Algorithm Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic.html">Basic and Utility Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ranking.html">Ranking Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bias.html">Bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="../knn.html">k-NN Collaborative Filtering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mf.html">Classic Matrix Factorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../addons.html">Add-On Algorithms</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tips and Tricks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance Tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diagnostics.html">Errors and Diagnostics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../impl-tips.html">Algorithm Implementation Tips</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Configuration and Internals</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../util.html">Utility Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internals.html">LensKit Internals</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">LensKit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> »</li>
          <li><a href="index.html">Evaluating Recommender Output</a> »</li>
      <li>Top-<em>N</em> Evaluation</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/lenskit/lkpy/blob/0.14.0/docs/evaluation/topn-metrics.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="top-n-evaluation">
<h1>Top-<em>N</em> Evaluation<a class="headerlink" href="#top-n-evaluation" title="Permalink to this headline"></a></h1>
<p>LensKit’s support for top-<em>N</em> evaluation is in two parts, because there are some
subtle complexities that make it more dfficult to get the right data in the right
place for computing metrics correctly.</p>
<section id="module-lenskit.topn">
<span id="top-n-analysis"></span><h2>Top-<em>N</em> Analysis<a class="headerlink" href="#module-lenskit.topn" title="Permalink to this headline"></a></h2>
<p>The <a class="reference internal" href="#module-lenskit.topn" title="lenskit.topn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">lenskit.topn</span></code></a> module contains the utilities for carrying out top-<em>N</em>
analysis, in conjucntion with <a class="reference internal" href="../batch.html#lenskit.batch.recommend" title="lenskit.batch.recommend"><code class="xref py py-func docutils literal notranslate"><span class="pre">lenskit.batch.recommend()</span></code></a>.</p>
<p>The entry point to this is <a class="reference internal" href="#lenskit.topn.RecListAnalysis" title="lenskit.topn.RecListAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">RecListAnalysis</span></code></a>.  This class encapsulates
an analysis with one or more metrics, and can apply it to data frames of recommendations.
An analysis requires two data frames: the recommendation frame contains the recommendations
themselves, and the truth frame contains the ground truth data for the users.  The
analysis is flexible with regards to the columns that identify individual recommendation
lists; usually these will consist of a user ID, data set identifier, and algorithm
identifier(s), but the analysis is configurable and its defaults make minimal assumptions.
The recommendation frame does need an <code class="docutils literal notranslate"><span class="pre">item</span></code> column with the recommended item IDs,
and it should be in order within a single recommendation list.</p>
<p>The truth frame should contain (a subset of) the columns identifying recommendation
lists, along with <code class="docutils literal notranslate"><span class="pre">item</span></code> and, if available, <code class="docutils literal notranslate"><span class="pre">rating</span></code> (if no rating is provided,
the metrics that need a rating value will assume a rating of 1 for every item present).
It can contain other items that custom metrics may find useful as well.</p>
<p>For example, a recommendation frame may contain:</p>
<ul class="simple">
<li><p>DataSet</p></li>
<li><p>Partition</p></li>
<li><p>Algorithm</p></li>
<li><p>user</p></li>
<li><p>item</p></li>
<li><p>rank</p></li>
<li><p>score</p></li>
</ul>
<p>And the truth frame:</p>
<ul class="simple">
<li><p>DataSet</p></li>
<li><p>user</p></li>
<li><p>item</p></li>
<li><p>rating</p></li>
</ul>
<p>The analysis will use this truth as the relevant item data for measuring the accuracy of the
roecommendation lists.  Recommendations will be matched to test ratings by data set, user,
and item, using <a class="reference internal" href="#lenskit.topn.RecListAnalysis" title="lenskit.topn.RecListAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">RecListAnalysis</span></code></a> defaults.</p>
<p>Identifying columns will be used to create two synthetic identifiers, <cite>LKRecID</cite> (the recommendation
list identifier) and <cite>LKTruthID</cite> (the truth list identifier), that are used in the internal data
frames.  Custom metric classes will see these on the data frames instead of other identifying columns.</p>
<dl class="py class">
<dt class="sig sig-object py" id="lenskit.topn.RecListAnalysis">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lenskit.topn.</span></span><span class="sig-name descname"><span class="pre">RecListAnalysis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">group_cols</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.topn.RecListAnalysis" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Compute one or more top-N metrics over recommendation lists.</p>
<p>This method groups the recommendations by the specified columns,
and computes the metric over each group.  The default set of grouping
columns is all columns <em>except</em> the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">item</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">score</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rating</span></code></p></li>
</ul>
<p>The truth frame, <code class="docutils literal notranslate"><span class="pre">truth</span></code>, is expected to match over (a subset of) the
grouping columns, and contain at least an <code class="docutils literal notranslate"><span class="pre">item</span></code> column.  If it also
contains a <code class="docutils literal notranslate"><span class="pre">rating</span></code> column, that is used as the users’ rating for
metrics that require it; otherwise, a rating value of 1 is assumed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>group_cols</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – The columns to group by, or <code class="docutils literal notranslate"><span class="pre">None</span></code> to use the default.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="lenskit.topn.RecListAnalysis.add_metric">
<span class="sig-name descname"><span class="pre">add_metric</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.topn.RecListAnalysis.add_metric" title="Permalink to this definition"></a></dt>
<dd><p>Add a metric to the analysis.</p>
<p>A metric is a function of two arguments: the a single group of the recommendation
frame, and the corresponding truth frame.  The truth frame will be indexed by
item ID.  The recommendation frame will be in the order in the data.  Many metrics
are defined in <a class="reference internal" href="#module-lenskit.metrics.topn" title="lenskit.metrics.topn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">lenskit.metrics.topn</span></code></a>; they are re-exported from
<a class="reference internal" href="#module-lenskit.topn" title="lenskit.topn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">lenskit.topn</span></code></a> for convenience.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric</strong> – The metric to compute.</p></li>
<li><p><strong>name</strong> – The name to assign the metric. If not provided, the function name is used.</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments to pass to the metric.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lenskit.topn.RecListAnalysis.compute">
<span class="sig-name descname"><span class="pre">compute</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_missing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.topn.RecListAnalysis.compute" title="Permalink to this definition"></a></dt>
<dd><p>Run the analysis.  Neither data frame should be meaningfully indexed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>recs</strong> (<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="(in pandas v1.4.1)"><em>pandas.DataFrame</em></a>) – A data frame of recommendations.</p></li>
<li><p><strong>truth</strong> (<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="(in pandas v1.4.1)"><em>pandas.DataFrame</em></a>) – A data frame of ground truth (test) data.</p></li>
<li><p><strong>include_missing</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> to include users from truth missing from recs.
Matches are done via group columns that appear in both
<code class="docutils literal notranslate"><span class="pre">recs</span></code> and <code class="docutils literal notranslate"><span class="pre">truth</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The results of the analysis.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame" title="(in pandas v1.4.1)">pandas.DataFrame</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-lenskit.metrics.topn">
<span id="metrics"></span><h2>Metrics<a class="headerlink" href="#module-lenskit.metrics.topn" title="Permalink to this headline"></a></h2>
<p>The <a class="reference internal" href="#module-lenskit.metrics.topn" title="lenskit.metrics.topn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">lenskit.metrics.topn</span></code></a> module contains metrics for evaluating top-<em>N</em>
recommendation lists.</p>
<p>Each of the top-<em>N</em> metrics supports an optional keyword argument <code class="docutils literal notranslate"><span class="pre">k</span></code> that specifies the expected list
length.  Recommendation lists are truncated to this length prior to measurment (so you can measure a
a metric at multiple values of <code class="docutils literal notranslate"><span class="pre">k</span></code> in a single analysis), and for recall-oriented metrics like
<a class="reference internal" href="#lenskit.metrics.topn.recall" title="lenskit.metrics.topn.recall"><code class="xref py py-func docutils literal notranslate"><span class="pre">recall()</span></code></a> and <a class="reference internal" href="#lenskit.metrics.topn.ndcg" title="lenskit.metrics.topn.ndcg"><code class="xref py py-func docutils literal notranslate"><span class="pre">ndcg()</span></code></a>, it normalizes the best-case possible items to <code class="docutils literal notranslate"><span class="pre">k</span></code> (because if
there are 10 relevant items, Recall@5 should be 1 when the list returns any 5 relevant items).
To use this, pass extra arguments to <code class="xref py py-meth docutils literal notranslate"><span class="pre">RecListAnalysis.add_metric()</span></code>:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">rla</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">ndcg</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">rla</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">ndcg</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'ndcg_10'</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>The default is to allow unbounded lists.  When using large recommendation lists, if users never have
more test ratings than there are recommended items, the default makes sense. For short recommendation
lists you will usually need to specify <code class="docutils literal notranslate"><span class="pre">k</span></code>.  Unfortunately, there isn’t a practical way to guess <code class="docutils literal notranslate"><span class="pre">k</span></code>,
because shorter lists may mean that the recommender could not produce a full-length list.</p>
<section id="classification-metrics">
<h3>Classification Metrics<a class="headerlink" href="#classification-metrics" title="Permalink to this headline"></a></h3>
<p>These metrics treat the recommendation list as a classification of relevant items.</p>
<dl class="py function">
<dt class="sig sig-object py" id="lenskit.metrics.topn.precision">
<span class="sig-prename descclassname"><span class="pre">lenskit.metrics.topn.</span></span><span class="sig-name descname"><span class="pre">precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.metrics.topn.precision" title="Permalink to this definition"></a></dt>
<dd><p>Compute recommendation precision.  This is computed as:</p>
<div class="math notranslate nohighlight">
\[\frac{|L \cap I_u^{\mathrm{test}}|}{|L|}\]</div>
<p>In the uncommon case that <code class="docutils literal notranslate"><span class="pre">k</span></code> is specified and <code class="docutils literal notranslate"><span class="pre">len(recs)</span> <span class="pre">&lt;</span> <span class="pre">k</span></code>, this metric uses
<code class="docutils literal notranslate"><span class="pre">len(recs)</span></code> as the denominator.</p>
<p>This metric has a bulk implementation.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lenskit.metrics.topn.recall">
<span class="sig-prename descclassname"><span class="pre">lenskit.metrics.topn.</span></span><span class="sig-name descname"><span class="pre">recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.metrics.topn.recall" title="Permalink to this definition"></a></dt>
<dd><p>Compute recommendation recall.  This is computed as:</p>
<div class="math notranslate nohighlight">
\[\frac{|L \cap I_u^{\mathrm{test}}|}{\operatorname{max}\{|I_u^{\mathrm{test}}|, k\}}\]</div>
<p>This metric has a bulk implementation.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lenskit.metrics.topn.hit">
<span class="sig-prename descclassname"><span class="pre">lenskit.metrics.topn.</span></span><span class="sig-name descname"><span class="pre">hit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.metrics.topn.hit" title="Permalink to this definition"></a></dt>
<dd><p>Compute whether or not a list is a hit; any list with at least one relevant item in the
first <span class="math notranslate nohighlight">\(k\)</span> positions (<span class="math notranslate nohighlight">\(L_{\le k} \cap I_u^{\mathrm{test}} \ne \emptyset\)</span>)
is scored as 1, and lists with no relevant items as 0.  When averaged over the recommendation
lists, this computes the <em>hit rate</em> <span id="id1">[<a class="reference internal" href="../references.html#id28" title="Mukund Deshpande and George Karypis. Item-based Top-N recommendation algorithms. ACM Transactions on Information Systems, 22(1):143–177, January 2004. URL: https://doi.org/10.1145/963770.963776, doi:10.1145/963770.963776.">DK04</a>]</span>.</p>
<div class="math notranslate nohighlight">
\[\frac{|L \cap I_u^{\mathrm{test}}|}{\operatorname{max}\{|I_u^{\mathrm{test}}|, k\}}\]</div>
<p>This metric has a bulk implementation.</p>
</dd></dl>

</section>
<section id="ranked-list-metrics">
<h3>Ranked List Metrics<a class="headerlink" href="#ranked-list-metrics" title="Permalink to this headline"></a></h3>
<p>These metrics treat the recommendation list as a ranked list of items that may or may not
be relevant.</p>
<dl class="py function">
<dt class="sig sig-object py" id="lenskit.metrics.topn.recip_rank">
<span class="sig-prename descclassname"><span class="pre">lenskit.metrics.topn.</span></span><span class="sig-name descname"><span class="pre">recip_rank</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.metrics.topn.recip_rank" title="Permalink to this definition"></a></dt>
<dd><p>Compute the reciprocal rank <span id="id2">[<a class="reference internal" href="../references.html#id12" title="Paul B Kantor and Ellen Voorhees. Report on the TREC-5 confusion track. In The Fifth Text REtrieval Conference (TREC-5). October 1997. URL: http://trec.nist.gov/pubs/trec5/t5_proceedings.html.">KV97</a>]</span> of the first relevant
item in a list of recommendations. Let <span class="math notranslate nohighlight">\(\kappa\)</span> denote the 1-based
rank of the first relevant item in <span class="math notranslate nohighlight">\(L\)</span>, with <span class="math notranslate nohighlight">\(\kappa=\infty\)</span>
if none of the first <span class="math notranslate nohighlight">\(k\)</span> items in <span class="math notranslate nohighlight">\(L\)</span> are relevant; then the
reciprocal rank is <span class="math notranslate nohighlight">\(1 / \kappa\)</span>. If no elements are relevant, the
reciprocal rank is therefore 0.  <span id="id3">Deshpande and Karypis [<a class="reference internal" href="../references.html#id28" title="Mukund Deshpande and George Karypis. Item-based Top-N recommendation algorithms. ACM Transactions on Information Systems, 22(1):143–177, January 2004. URL: https://doi.org/10.1145/963770.963776, doi:10.1145/963770.963776.">DK04</a>]</span> call this the
“reciprocal hit rate”.</p>
<p>This metric has a bulk equivalent.</p>
</dd></dl>

</section>
<section id="utility-metrics">
<h3>Utility Metrics<a class="headerlink" href="#utility-metrics" title="Permalink to this headline"></a></h3>
<p>The NDCG function estimates a utility score for a ranked list of recommendations.</p>
<dl class="py function">
<dt class="sig sig-object py" id="lenskit.metrics.topn.ndcg">
<span class="sig-prename descclassname"><span class="pre">lenskit.metrics.topn.</span></span><span class="sig-name descname"><span class="pre">ndcg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount=&lt;ufunc</span> <span class="pre">'log2'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k=None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.metrics.topn.ndcg" title="Permalink to this definition"></a></dt>
<dd><p>Compute the normalized discounted cumulative gain <span id="id4">[<a class="reference internal" href="../references.html#id26" title="Kalervo Järvelin and Jaana Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422–446, October 2002. URL: https://doi.org/10.1145/582415.582418, doi:10.1145/582415.582418.">JarvelinKekalainen02</a>]</span>.</p>
<p>Discounted cumultative gain is computed as:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}
\mathrm{DCG}(L,u) &amp; = \sum_{i=1}^{|L|} \frac{r_{ui}}{d(i)}
\end{align*}\]</div>
<p>Unrated items are assumed to have a utility of 0; if no rating values are provided in the
truth frame, item ratings are assumed to be 1.</p>
<p>This is then normalized as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}
\mathrm{nDCG}(L, u) &amp; = \frac{\mathrm{DCG}(L,u)}{\mathrm{DCG}(L_{\mathrm{ideal}}, u)}
\end{align*}\]</div>
<p>This metric has a bulk implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>recs</strong> – The recommendation list.</p></li>
<li><p><strong>truth</strong> – The user’s test data.</p></li>
<li><p><strong>discount</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ufunc.html#numpy.ufunc" title="(in NumPy v1.22)"><em>numpy.ufunc</em></a>) – The rank discount function.  Each item’s score will be divided the discount of its rank,
if the discount is greater than 1.</p></li>
<li><p><strong>k</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The maximum list length.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lenskit.metrics.topn.dcg">
<span class="sig-prename descclassname"><span class="pre">lenskit.metrics.topn.</span></span><span class="sig-name descname"><span class="pre">dcg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount=&lt;ufunc</span> <span class="pre">'log2'&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.metrics.topn.dcg" title="Permalink to this definition"></a></dt>
<dd><p>Compute the <strong>unnormalized</strong> discounted cumulative gain <span id="id5">[<a class="reference internal" href="../references.html#id26" title="Kalervo Järvelin and Jaana Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422–446, October 2002. URL: https://doi.org/10.1145/582415.582418, doi:10.1145/582415.582418.">JarvelinKekalainen02</a>]</span>.</p>
<p>Discounted cumultative gain is computed as:</p>
<div class="math notranslate nohighlight">
\[\begin{align*}
\mathrm{DCG}(L,u) &amp; = \sum_{i=1}^{|L|} \frac{r_{ui}}{d(i)}
\end{align*}\]</div>
<p>Unrated items are assumed to have a utility of 0; if no rating values are provided in the
truth frame, item ratings are assumed to be 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>recs</strong> – The recommendation list.</p></li>
<li><p><strong>truth</strong> – The user’s test data.</p></li>
<li><p><strong>discount</strong> (<em>ufunc</em>) – The rank discount function.  Each item’s score will be divided the discount of its rank,
if the discount is greater than 1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>We also expose the internal DCG computation directly.</p>
<dl class="py function">
<dt class="sig sig-object py" id="lenskit.metrics.topn._dcg">
<span class="sig-prename descclassname"><span class="pre">lenskit.metrics.topn.</span></span><span class="sig-name descname"><span class="pre">_dcg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scores</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount=&lt;ufunc</span> <span class="pre">'log2'&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.metrics.topn._dcg" title="Permalink to this definition"></a></dt>
<dd><p>Compute the Discounted Cumulative Gain of a series of recommended items with rating scores.
These should be relevance scores; they can be <span class="math notranslate nohighlight">\({0,1}\)</span> for binary relevance data.</p>
<p>This is not a true top-N metric, but is a utility function for other metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scores</strong> (<em>array-like</em>) – The utility scores of a list of recommendations, in recommendation order.</p></li>
<li><p><strong>discount</strong> (<em>ufunc</em>) – the rank discount function.  Each item’s score will be divided the discount of its rank,
if the discount is greater than 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the DCG of the scored items.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>double</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="writing-a-metric">
<h3>Writing a Metric<a class="headerlink" href="#writing-a-metric" title="Permalink to this headline"></a></h3>
<p>A metric is a function that takes two positional parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">recs</span></code>, a data frame of recommendations for a single recommendation list.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">truth</span></code>, a data frame of ground-truth data (usually ratings) for the user for whom the list was generated.</p></li>
</ul>
<p>It can take additional keyword arguments that are passed through from <code class="xref py py-meth docutils literal notranslate"><span class="pre">RecListAnalysis.add_metric()</span></code>.
A metric then returns a single floating-point value; NaN is allowed.</p>
<p>Metrics can be further optimized with the <em>bulk interface</em>.  A bulk metric function takes <code class="docutils literal notranslate"><span class="pre">recs</span></code>
and <code class="docutils literal notranslate"><span class="pre">truth</span></code> frames for the <em>entire set of recommendations</em>, with transformation (they have
<code class="docutils literal notranslate"><span class="pre">LKRecID</span></code> and <code class="docutils literal notranslate"><span class="pre">LKTruthID</span></code> columns instead of other identifying columns), and returns a series
whose index is <code class="docutils literal notranslate"><span class="pre">LKRecID</span></code> and values are the metric values for each list.  Further, the <code class="docutils literal notranslate"><span class="pre">recs</span></code>
passed to a bulk implementation includes a 1-based <em>rank</em> for each recommendation.</p>
<p>The <a class="reference internal" href="#lenskit.metrics.topn.bulk_impl" title="lenskit.metrics.topn.bulk_impl"><code class="xref py py-func docutils literal notranslate"><span class="pre">bulk_impl()</span></code></a> function registers a bulk implementation of a metric:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">metric</span><span class="p">(</span><span class="n">recs</span><span class="p">,</span> <span class="n">truth</span><span class="p">):</span>
    <span class="c1"># normal metric implementation</span>
    <span class="k">pass</span>

<span class="nd">@bulk_impl</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_bulk_metric</span><span class="p">(</span><span class="n">recs</span><span class="p">,</span> <span class="n">truth</span><span class="p">):</span>
    <span class="c1"># bulk metric implementation</span>
</pre></div>
</div>
<p>If a bulk implementation of a metric is available, and it is possible to use it, it will be used automatically
when the corresponding metric is passed to <a class="reference internal" href="#lenskit.topn.RecListAnalysis.add_metric" title="lenskit.topn.RecListAnalysis.add_metric"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_metric()</span></code></a>.</p>
<dl class="py function">
<dt class="sig sig-object py" id="lenskit.metrics.topn.bulk_impl">
<span class="sig-prename descclassname"><span class="pre">lenskit.metrics.topn.</span></span><span class="sig-name descname"><span class="pre">bulk_impl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#lenskit.metrics.topn.bulk_impl" title="Permalink to this definition"></a></dt>
<dd><p>Decorator to register a bulk implementation for a metric.</p>
</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="predict-metrics.html" class="btn btn-neutral float-left" title="Prediction Accuracy Metrics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../interfaces.html" class="btn btn-neutral float-right" title="Algorithm Interfaces" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr>

  <div role="contentinfo">
    <p>© Copyright 2018–2019 Boise State University.
      <span class="commit">Revision <code>9e471a65</code>.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
<p>This material is based upon work supported by the National Science Foundation under
Grant No. <a href="https://md.ekstrandom.net/research/career">IIS 17-51278</a>. Any
opinions, findings, and conclusions or recommendations expressed in this material
are those of the author(s) and do not necessarily reflect the views of the
National Science Foundation.  This page has not been approved by
Boise State University and does not reflect official university positions.</p>
<script data-goatcounter="https://lenskit.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>


</footer>
        </div>
      </div>
    </section>
  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="Versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: 0.14.0
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
          <dd><a href="/en/stable/">stable</a></dd>
        
          <dd><a href="/en/0.14.0/">0.14.0</a></dd>
        
          <dd><a href="/en/0.13.1/">0.13.1</a></dd>
        
          <dd><a href="/en/0.13.0/">0.13.0</a></dd>
        
          <dd><a href="/en/0.12.3/">0.12.3</a></dd>
        
          <dd><a href="/en/0.12.2/">0.12.2</a></dd>
        
          <dd><a href="/en/0.12.1/">0.12.1</a></dd>
        
          <dd><a href="/en/0.12.0/">0.12.0</a></dd>
        
          <dd><a href="/en/0.11.1/">0.11.1</a></dd>
        
          <dd><a href="/en/0.11.0/">0.11.0</a></dd>
        
          <dd><a href="/en/0.10.1/">0.10.1</a></dd>
        
          <dd><a href="/en/0.10.0/">0.10.0</a></dd>
        
          <dd><a href="/en/0.9.0/">0.9.0</a></dd>
        
          <dd><a href="/en/0.8.4/">0.8.4</a></dd>
        
          <dd><a href="/en/0.8.1/">0.8.1</a></dd>
        
          <dd><a href="/en/0.8.0/">0.8.0</a></dd>
        
          <dd><a href="/en/0.7.0/">0.7.0</a></dd>
        
          <dd><a href="/en/0.6.1/">0.6.1</a></dd>
        
          <dd><a href="/en/0.6.0/">0.6.0</a></dd>
        
          <dd><a href="/en/0.5.0/">0.5.0</a></dd>
        
          <dd><a href="/en/0.3.0/">0.3.0</a></dd>
        
          <dd><a href="/en/0.2.0/">0.2.0</a></dd>
        
          <dd><a href="/en/0.1.0/">0.1.0</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
      </dl>
      
    </div>
  </div>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 


</body></html>